{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9a29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing mixture of experts layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ddf7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder_layer import DecoderLayer,DecoderLayer_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2465272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export pythonpath to make it work!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc750258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input dimension can be 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc059865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61db0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt: tensor([[[-0.3517, -0.6832, -1.0543,  1.4223,  0.4189,  0.2320,  0.5124,\n",
      "           1.7014],\n",
      "         [-0.0648, -1.2922, -0.9240, -0.8955,  0.6399, -0.2751,  0.6975,\n",
      "           0.7029],\n",
      "         [-0.4311,  0.2497,  0.6735,  1.0755, -0.1709,  0.0925, -0.5111,\n",
      "          -1.0584],\n",
      "         [-0.9791, -0.8657, -0.9515,  0.9604, -0.4255, -0.3007,  1.2598,\n",
      "          -0.4309]],\n",
      "\n",
      "        [[-0.8677, -0.4915, -0.3486, -0.2433,  0.6749,  0.3176, -1.2774,\n",
      "          -0.5385],\n",
      "         [ 0.2985, -0.1175, -0.6608,  1.8647, -3.4407,  1.9859,  0.1879,\n",
      "          -0.9056],\n",
      "         [-0.1621, -0.4816,  0.1278, -0.2487,  0.3038, -1.0555,  1.3565,\n",
      "          -1.8000],\n",
      "         [-0.6641, -0.6413,  1.8404, -1.8469,  0.1313, -1.7509, -0.1396,\n",
      "          -1.3058]]])\n",
      "tgt_mask: tensor([[True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "memory: tensor([[[ 0.0526,  0.8739, -0.7208, -1.5964, -0.6504, -0.3283,  0.6652,\n",
      "          -0.5320],\n",
      "         [-0.3661, -2.5825,  0.0792, -0.0296, -0.5748, -1.5106, -1.3222,\n",
      "          -0.3494],\n",
      "         [ 0.4396, -0.1107,  0.0285, -0.2160, -1.9062,  0.8145,  0.8123,\n",
      "           1.7270],\n",
      "         [ 2.0608,  0.4693, -2.1240,  0.4793,  1.6899, -1.7320,  0.0585,\n",
      "           1.0117]],\n",
      "\n",
      "        [[-0.4416, -0.1595, -0.3624,  0.8399, -0.1656,  0.6437,  0.0126,\n",
      "          -0.0503],\n",
      "         [ 2.3066,  1.4225,  0.0787, -2.5321,  0.2123, -0.2850, -0.5443,\n",
      "           0.6866],\n",
      "         [-0.1565,  1.0500, -0.6069,  0.5748, -0.1228,  2.3940, -0.1756,\n",
      "           1.1012],\n",
      "         [ 1.2645, -2.3478,  0.5738,  2.4829,  1.8237, -0.5557, -0.2951,\n",
      "           0.8645]]])\n",
      "memory_mask: tensor([[True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "cache: tensor([[[ 0.0598, -1.1729, -1.2634,  2.2927,  1.3227, -0.8987, -0.2934,\n",
      "           0.1621],\n",
      "         [-1.7129, -1.0580, -1.2707, -1.4844,  1.2704,  0.2535,  1.1816,\n",
      "           0.5410],\n",
      "         [-0.5416,  0.7082,  0.6735,  0.7400, -0.6631, -1.0798, -0.2595,\n",
      "          -0.6129]],\n",
      "\n",
      "        [[ 1.8096,  0.2381, -0.2718,  0.1149,  0.1016,  1.5388,  1.2642,\n",
      "           1.3058],\n",
      "         [ 0.1195,  0.9975, -0.4667,  2.4440, -0.0508,  1.7401, -0.2054,\n",
      "          -0.3735],\n",
      "         [ 1.9181, -0.8122,  0.2893,  0.8855,  1.1095,  1.0358,  0.3486,\n",
      "           0.6795]]])\n",
      "pre_memory: tensor([[[-0.1267,  1.0185,  1.3182,  0.5423,  1.3330, -1.0853, -0.3874,\n",
      "          -0.2053],\n",
      "         [ 0.3025,  0.5018,  0.1298,  0.5790,  0.0756,  0.0637, -1.0247,\n",
      "           0.8671],\n",
      "         [-0.3519, -0.8897, -0.6619, -0.3445, -0.2172,  0.1880,  0.2226,\n",
      "           0.6530],\n",
      "         [-0.0541, -0.8494, -0.6342,  0.4768, -0.6898,  0.5593,  0.7376,\n",
      "          -0.0155]],\n",
      "\n",
      "        [[-0.0158, -1.3262, -1.1939,  0.9778,  1.3850, -2.1369, -0.6029,\n",
      "           0.4924],\n",
      "         [-2.1786, -0.7492,  0.0641, -0.4674,  0.9204,  0.4434, -1.0280,\n",
      "           0.5334],\n",
      "         [-0.4595,  0.5682,  0.5503,  1.0821,  0.7196, -0.3271, -0.6460,\n",
      "          -1.9516],\n",
      "         [-0.3309,  0.6498, -0.2446, -1.7386, -0.8555,  1.5949, -1.4863,\n",
      "           1.6373]]])\n",
      "pre_memory_mask: tensor([[True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example values\n",
    "batch_size = 2\n",
    "seq_len_tgt = 4\n",
    "seq_len_mem = 4\n",
    "hidden_size = 8\n",
    "\n",
    "# Random tensor for target input (tgt)\n",
    "tgt = torch.randn(batch_size, seq_len_tgt, hidden_size)\n",
    "print(\"tgt:\", tgt)\n",
    "\n",
    "# Example mask for target input (tgt_mask)\n",
    "tgt_mask = torch.ones(batch_size, seq_len_tgt, dtype=torch.bool)\n",
    "print(\"tgt_mask:\", tgt_mask)\n",
    "\n",
    "# Random tensor for memory (output from the encoder)\n",
    "memory = torch.randn(batch_size, seq_len_mem, hidden_size)\n",
    "print(\"memory:\", memory)\n",
    "\n",
    "# Example mask for memory (memory_mask)\n",
    "memory_mask = torch.ones(batch_size, seq_len_mem, dtype=torch.bool)\n",
    "print(\"memory_mask:\", memory_mask)\n",
    "\n",
    "# Example cache (previous decoder states)\n",
    "cache = torch.randn(batch_size, seq_len_tgt - 1, hidden_size)\n",
    "print(\"cache:\", cache)\n",
    "\n",
    "# Random tensor for pre_memory (if using sequential attention)\n",
    "pre_memory = torch.randn(batch_size, seq_len_mem, hidden_size)\n",
    "print(\"pre_memory:\", pre_memory)\n",
    "\n",
    "# Example mask for pre_memory (pre_memory_mask)\n",
    "pre_memory_mask = torch.ones(batch_size, seq_len_mem, dtype=torch.bool)\n",
    "print(\"pre_memory_mask:\", pre_memory_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f539ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleMultiHeadedAttention(nn.Module):\n",
    "#     def forward(self, query, key, value, mask):\n",
    "#         return query + key + value  # This is just a placeholder\n",
    "\n",
    "# # Instantiate the self-attention and source-attention modules\n",
    "# self_attn = SimpleMultiHeadedAttention()\n",
    "# src_attn = SimpleMultiHeadedAttention()\n",
    "\n",
    "# # Instantiate the DecoderLayer\n",
    "# decoder_layer = DecoderLayer(\n",
    "#     size=hidden_size,\n",
    "#     self_attn=self_attn,\n",
    "#     src_attn=src_attn,\n",
    "#     dropout_rate=0.1,\n",
    "#     normalize_before=True,\n",
    "#     concat_after=False,\n",
    "#     sequential_attn=None\n",
    "# )\n",
    "\n",
    "# # Initialize example inputs\n",
    "# tgt = torch.randn(batch_size, seq_len_tgt, hidden_size)\n",
    "# tgt_mask = torch.ones(batch_size, seq_len_tgt, dtype=torch.bool)\n",
    "# memory = torch.randn(batch_size, seq_len_mem, hidden_size)\n",
    "# memory_mask = torch.ones(batch_size, seq_len_mem, dtype=torch.bool)\n",
    "# cache = torch.randn(batch_size, seq_len_tgt - 1, hidden_size)\n",
    "# pre_memory = torch.randn(batch_size, seq_len_mem, hidden_size)\n",
    "# pre_memory_mask = torch.ones(batch_size, seq_len_mem, dtype=torch.bool)\n",
    "\n",
    "# # Run a forward pass\n",
    "# output = decoder_layer(\n",
    "#     tgt=tgt,\n",
    "#     tgt_mask=tgt_mask,\n",
    "#     memory=memory,\n",
    "#     memory_mask=memory_mask,\n",
    "#     cache=cache,\n",
    "#     pre_memory=pre_memory,\n",
    "#     pre_memory_mask=pre_memory_mask\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d8c56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The way to figure this out is to make sure the output of single pointwise ffn is same as output of moe layer. That is it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93f5c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class SMoE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts):\n",
    "        super(SMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        self.gating_network = GatingNetwork(input_dim, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate_scores = self.gating_network(x)\n",
    "        topk_scores, topk_indices = torch.topk(gate_scores, 2, dim=-1)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        expert_outputs = torch.zeros_like(x)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                indices = topk_indices[i, j]\n",
    "                weights = F.softmax(topk_scores[i, j], dim=-1)\n",
    "                output = sum(weights[k] * self.experts[indices[k].item()](x[i:i+1, j:j+1, :]) for k in range(2))\n",
    "                expert_outputs[i:i+1, j:j+1, :] = output\n",
    "        \n",
    "        return expert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c558b31a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.1):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "input_dim = 8\n",
    "hidden_dim = 32\n",
    "\n",
    "# Create an instance of the PointWiseFeedForward network\n",
    "ffn = PointWiseFeedForward(input_dim, hidden_dim)\n",
    "\n",
    "# Example input tensor\n",
    "x = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = ffn(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fd68c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "sparse_moe = SMoE(input_dim, hidden_dim=hidden_dim,num_experts=8)\n",
    "output = sparse_moe(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05a57a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class SMoE1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts):\n",
    "        super(SMoE1, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        self.gating_network = GatingNetwork(input_dim, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate_scores = self.gating_network(x)\n",
    "        topk_scores, topk_indices = torch.topk(gate_scores, 2, dim=-1)\n",
    "        print(f\"topk_indices:{topk_indices}\")\n",
    "        topk_experts = [self.experts[idx.item()](x) for idx in topk_indices]\n",
    "        \n",
    "        gate_weights = F.softmax(topk_scores, dim=-1)\n",
    "        output = sum(weight.unsqueeze(-1) * expert_output for weight, expert_output in zip(gate_weights, topk_experts))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2affa78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_moe1 = SMoE1(input_dim, hidden_dim=hidden_dim,num_experts=8)\n",
    "# output = sparse_moe1(x)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing faster Feed forward network.\n",
    "\n",
    "#We must have same output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab46431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_size:6,Seq_len:6,k:2\n",
      "36\n",
      "tensor(894.6484, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Observing load_loss\n",
    "import torch\n",
    "\n",
    "def load_loss(topk_values, topk_indices, num_experts):\n",
    "    \"\"\"\n",
    "    Calculate the load loss based on the distribution of loads among experts.\n",
    "    \n",
    "    Args:\n",
    "        topk_values (torch.Tensor): Tensor containing the top k values after gating. Shape (batch_size, seq_len, k)\n",
    "        topk_indices (torch.Tensor): Tensor containing the indices of the experts corresponding to top k values. Shape (batch_size, seq_len, k)\n",
    "        num_experts (int): The total number of experts.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The calculated load loss.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, k = topk_values.shape\n",
    "    num_elements = batch_size * seq_len  # |X|\n",
    "    print(f\"Batch_size:{batch_size},Seq_len:{seq_len},k:{k}\")\n",
    "    # Initialize the load for each expert\n",
    "    expert_loads = torch.zeros(num_experts, device=topk_values.device)\n",
    "    \n",
    "    # Calculate the load for each expert\n",
    "    for i in range(num_experts):\n",
    "        expert_loads[i] = (topk_indices == i).sum().item()\n",
    "    \n",
    "    # Calculate the logits sum for each expert\n",
    "    logits_sum = torch.zeros(num_experts, device=topk_values.device)\n",
    "    for p in range(num_experts):\n",
    "        mask = topk_indices == p  # Mask to select the logits corresponding to expert p\n",
    "        logits_sum[p] = (topk_values * mask.float()).sum().item()\n",
    "    # Calculate the load loss\n",
    "    load_loss_value = (num_experts / num_elements) * (expert_loads * logits_sum).sum()\n",
    "    \n",
    "    return load_loss_value\n",
    "\n",
    "# Example usage\n",
    "topk_values = torch.tensor([[[3.0781, 3.0156],\n",
    "                             [4.7812, 4.1875],\n",
    "                             [2.2656, 2.0000],\n",
    "                             [3.1562, 1.6641],\n",
    "                             [2.3750, 1.6250],\n",
    "                             [3.9531, 2.4062]],\n",
    "\n",
    "                            [[2.2812, 2.1094],\n",
    "                             [2.5938, 2.4688],\n",
    "                             [3.5625, 1.5234],\n",
    "                             [3.1719, 1.5391],\n",
    "                             [4.2188, 1.1953],\n",
    "                             [4.0625, 1.2031]],\n",
    "\n",
    "                            [[4.5000, 2.5469],\n",
    "                             [1.0625, 0.9609],\n",
    "                             [2.0469, 1.4375],\n",
    "                             [4.3750, 1.0234],\n",
    "                             [4.4375, 1.2656],\n",
    "                             [3.6250, 1.2031]],\n",
    "\n",
    "\n",
    "                            [[4.0312, 3.3125],\n",
    "                             [3.0156, 2.1875],\n",
    "                             [1.3516, 1.2109],\n",
    "                             [4.5312, 4.5000],\n",
    "                             [4.0312, 3.7031],\n",
    "                             [5.0625, 3.6562]],\n",
    "\n",
    "                            [[4.4688, 0.8633],\n",
    "                             [2.4844, 1.5625],\n",
    "                             [2.1875, 1.8984],\n",
    "                             [4.8438, 0.8516],\n",
    "                             [4.3438, 0.8398],\n",
    "                             [4.1250, 0.9844]],\n",
    "\n",
    "                            [[3.4062, 2.0625],\n",
    "                             [2.5156, 1.3203],\n",
    "                             [1.2656, 1.2500],\n",
    "                             [3.5625, 3.1094],\n",
    "                             [3.6875, 3.4062],\n",
    "                             [3.8281, 3.3438]]], device='cuda:0', dtype=torch.bfloat16)\n",
    "\n",
    "topk_indices = torch.tensor([[[5, 7],\n",
    "                              [2, 1],\n",
    "                              [0, 1],\n",
    "                              [0, 1],\n",
    "                              [3, 5],\n",
    "                              [5, 7]],\n",
    "\n",
    "                             [[2, 5],\n",
    "                              [5, 3],\n",
    "                              [7, 3],\n",
    "                              [5, 1],\n",
    "                              [5, 3],\n",
    "                              [5, 1]],\n",
    "\n",
    "                             [[1, 5],\n",
    "                              [2, 1],\n",
    "                              [7, 2],\n",
    "                              [5, 1],\n",
    "                              [5, 1],\n",
    "                              [5, 3]],\n",
    "\n",
    "                             [[5, 1],\n",
    "                              [5, 7],\n",
    "                              [3, 5],\n",
    "                              [1, 5],\n",
    "                              [1, 5],\n",
    "                              [1, 5]],\n",
    "\n",
    "                             [[5, 3],\n",
    "                              [7, 5],\n",
    "                              [0, 1],\n",
    "                              [5, 7],\n",
    "                              [5, 7],\n",
    "                              [5, 7]],\n",
    "\n",
    "                             [[1, 5],\n",
    "                              [5, 3],\n",
    "                              [0, 5],\n",
    "                              [5, 1],\n",
    "                              [1, 5],\n",
    "                              [1, 5]]], device='cuda:0')\n",
    "\n",
    "num_experts = 8\n",
    "loss = load_loss(topk_values, topk_indices, num_experts)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03362569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, ModuleList\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from beartype import beartype\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Tuple, Union\n",
    "from einops import rearrange, repeat, reduce, pack, unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba21ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay let's try out st-moe implementation. They used einstein notation. This is important to learn right now.\n",
    "class TopNGating(Module):\n",
    "\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_gates,\n",
    "        eps = 1e-9,\n",
    "        top_n = 2,\n",
    "        threshold_train: Union[float, Tuple[float, ...]] = 0.2,\n",
    "        threshold_eval: Union[float, Tuple[float, ...]] = 0.2,\n",
    "        capacity_factor_train = 1.25,\n",
    "        capacity_factor_eval = 2.,\n",
    "        straight_through_dispatch_tensor = True,\n",
    "        differentiable_topk = False,\n",
    "        differentiable_topk_fused = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.num_gates = num_gates\n",
    "        self.to_gates = nn.Linear(dim, num_gates, bias = False)\n",
    "\n",
    "        self.differentiable_topk = differentiable_topk\n",
    "\n",
    "        self.topk = partial(\n",
    "            maybe_differentiable_topk,\n",
    "            non_differentiable = not differentiable_topk,\n",
    "            fused = differentiable_topk_fused # use triton fused coordinate descent if possible by default\n",
    "        )\n",
    "\n",
    "        assert top_n >= 2, 'must be 2 or more experts'\n",
    "        self.top_n = top_n\n",
    "        top_n_minus_1 = top_n - 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        noise_gates = False,\n",
    "        noise_mult = 1.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        einstein notation:\n",
    "\n",
    "        b - batch\n",
    "        n - sequence\n",
    "        e - experts\n",
    "        k - top-n experts\n",
    "        \"\"\"\n",
    "\n",
    "        *_, b, group_size, dim, dtype, top_n, num_gates, eps = *x.shape, x.dtype, self.top_n, self.num_gates, self.eps\n",
    "\n",
    "        # threshold, capacity depending on training or eval\n",
    "\n",
    "        suffix = 'train' if self.training else 'eval'\n",
    "\n",
    "        threshold = getattr(self, f'threshold_{suffix}')\n",
    "        capacity_factor = getattr(self, f'capacity_factor_{suffix}')\n",
    "\n",
    "        # Each sequence sends (at most?) expert_capacity positions to each expert.\n",
    "        # Static expert_capacity dimension is needed for expert batch sizes\n",
    "\n",
    "        expert_capacity = min(group_size, int((group_size * capacity_factor) / num_gates))\n",
    "        expert_capacity = max(expert_capacity, MIN_EXPERT_CAPACITY)\n",
    "        expert_capacity_f = float(expert_capacity)\n",
    "\n",
    "        # gate logits and gates\n",
    "\n",
    "        gate_logits = self.to_gates(x)\n",
    "\n",
    "        maybe_noised_gate_logits = gate_logits\n",
    "\n",
    "        if noise_gates:\n",
    "            noise = gumbel_noise(maybe_noised_gate_logits)\n",
    "            maybe_noised_gate_logits = maybe_noised_gate_logits + noise * noise_mult\n",
    "\n",
    "        raw_gates = maybe_noised_gate_logits.softmax(dim = -1)\n",
    "\n",
    "        # find top N experts per position\n",
    "\n",
    "        topk_return = self.topk(raw_gates, k = top_n)\n",
    "\n",
    "        gate_indices = topk_return.indices\n",
    "\n",
    "        if self.differentiable_topk:\n",
    "            # allow for differentiable topk using coordinate descent\n",
    "            # used successfully for routing from CoLT5 paper https://github.com/lucidrains/CoLT5-attention\n",
    "\n",
    "            gates = topk_return.coor_descent_values\n",
    "        else:\n",
    "            gates = topk_return.values\n",
    "\n",
    "        # move the top-n dimension to be first\n",
    "\n",
    "        gates = rearrange(gates, '... k -> k ...')\n",
    "        gate_indices = rearrange(gate_indices, '... k -> k ...')\n",
    "\n",
    "        # masks\n",
    "\n",
    "        one_hot_gate_indices = F.one_hot(gate_indices, num_gates)\n",
    "        mask = one_hot_gate_indices.float()\n",
    "\n",
    "        mask_1 = mask[0] # needed for balancing loss\n",
    "\n",
    "        # normalize top-n gate scores\n",
    "\n",
    "        denom = reduce(gates, 'k ... -> 1 ...', 'sum').clamp(min = eps)\n",
    "        gates = gates / denom\n",
    "\n",
    "        # best performing policy was to route to the second expert, with probability of min(1., score / threshold), where score = gate2 / (gate1 + gate2)\n",
    "        # optimal threshold was ~ 0.2\n",
    "        # generalized to more than 2 experts\n",
    "\n",
    "        probs = torch.zeros_like(gates).uniform_(0., 1.)\n",
    "\n",
    "        threshold = rearrange(threshold, 'k -> k 1 1')\n",
    "        should_route = probs < (gates / threshold.clamp(min = eps))\n",
    "\n",
    "        # tokens should always be routed to first expert\n",
    "        # threshold for first expert already set to very small number, but just in case\n",
    "\n",
    "        should_route[0, ...] = True\n",
    "\n",
    "        mask *= rearrange(should_route.float(), '... -> ... 1')\n",
    "\n",
    "        mask_cumsum = cumsum_exclusive(mask, dim = -2) # along sequence dimension\n",
    "\n",
    "        # compute assignment to experts - (batch, seq, experts)\n",
    "\n",
    "        # This is the position within the expert's mini-batch for this sequence\n",
    "\n",
    "        positions = []\n",
    "        prev_expert_count = 0.\n",
    "\n",
    "        for n in range(self.top_n):\n",
    "            position_in_expert = (mask_cumsum[n] + prev_expert_count) * mask[n]\n",
    "\n",
    "            # Remove the elements that don't fit. (batch, sequence, experts)\n",
    "            mask[n] *= (position_in_expert < expert_capacity_f).float()\n",
    "\n",
    "            # How many examples in this sequence go to this expert - needed for the next iteration as offset\n",
    "            prev_expert_count = reduce(mask[n], '... n e -> ... 1 e', 'sum')\n",
    "\n",
    "            # (batch, sequence)\n",
    "            position_in_expert = reduce(position_in_expert, '... n e -> ... n', 'sum')\n",
    "            positions.append(position_in_expert)\n",
    "\n",
    "        positions = torch.stack(positions)\n",
    "\n",
    "        # (k, batch, sequence) - mostly ones, but zeros where something didn't fit\n",
    "        mask_flat = reduce(mask, '... n e -> ... n', 'sum')\n",
    "\n",
    "        # (k, batch, sequence) - weighted assignment\n",
    "        # following https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py#L1903\n",
    "        gates = gates * mask_flat\n",
    "\n",
    "        # (batch, sequence, experts, expert_capacity)\n",
    "\n",
    "        N = None\n",
    "\n",
    "        gates = gates[..., N, N]\n",
    "        mask_flat = mask_flat[..., N, N]\n",
    "        one_hot_gate_indices = one_hot_gate_indices[..., N]\n",
    "        safe_one_hot_gates = safe_one_hot(positions.long(), expert_capacity)[..., N, :]\n",
    "\n",
    "        combine_tensor = reduce(\n",
    "            gates\n",
    "            * mask_flat\n",
    "            * one_hot_gate_indices\n",
    "            * safe_one_hot_gates\n",
    "        , 'k ... -> ...', 'sum')\n",
    "\n",
    "        # dispatch tensor\n",
    "\n",
    "        dispatch_tensor = combine_tensor.bool().type(dtype)\n",
    "\n",
    "        if self.straight_through_dispatch_tensor:\n",
    "            dispatch_tensor = dispatch_tensor + combine_tensor - combine_tensor.detach()\n",
    "\n",
    "        # balance losses - (batch, experts)\n",
    "        # We want to equalize the fraction of the batch assigned to each expert\n",
    "\n",
    "        if self.training:\n",
    "            density_1 = reduce(mask_1, '... n e -> ... e', 'mean')\n",
    "            density_1_proxy = reduce(raw_gates, '... n e -> ... e', 'mean') # Something continuous that is correlated with what we want to equalize.\n",
    "\n",
    "            balance_loss = (density_1_proxy * density_1).mean() * float(num_gates ** 2)\n",
    "        else:\n",
    "            balance_loss = self.zero\n",
    "\n",
    "        # calculate the router z-loss proposed in paper\n",
    "\n",
    "        if self.training:\n",
    "            router_z_loss = torch.logsumexp(gate_logits, dim = -1)\n",
    "            router_z_loss = torch.square(router_z_loss)            \n",
    "            router_z_loss = router_z_loss.mean()\n",
    "        else:\n",
    "            router_z_loss = self.zero\n",
    "\n",
    "        return dispatch_tensor, combine_tensor, balance_loss, router_z_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5de3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(Module):\n",
    "\n",
    "    @beartype\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        num_experts = 16,\n",
    "        expert_hidden_mult = 4,\n",
    "        threshold_train = 0.2,\n",
    "        threshold_eval = 0.2,\n",
    "        capacity_factor_train = 1.25,\n",
    "        capacity_factor_eval = 2.,\n",
    "        gating_top_n = 2,\n",
    "        balance_loss_coef = 1e-2,\n",
    "        router_z_loss_coef = 1e-3,\n",
    "        experts: Optional[Module] = None,\n",
    "        straight_through_dispatch_tensor = True,\n",
    "        differentiable_topk = False,\n",
    "        differentiable_topk_fused = True,\n",
    "        is_distributed = None,\n",
    "        allow_var_seq_len = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        self.gate = TopNGating(\n",
    "            dim,\n",
    "            top_n = gating_top_n,\n",
    "            num_gates = num_experts,\n",
    "            straight_through_dispatch_tensor = straight_through_dispatch_tensor,\n",
    "            differentiable_topk = differentiable_topk,\n",
    "            threshold_train = threshold_train,\n",
    "            threshold_eval = threshold_eval,\n",
    "            capacity_factor_train = capacity_factor_train,\n",
    "            capacity_factor_eval = capacity_factor_eval\n",
    "        )\n",
    "\n",
    "        experts = default(experts, lambda: [Expert(dim = dim, hidden_mult = expert_hidden_mult) for _ in range(num_experts)])\n",
    "\n",
    "        self.experts = Experts(\n",
    "            experts,\n",
    "            is_distributed = is_distributed,\n",
    "            allow_var_seq_len = allow_var_seq_len\n",
    "        )\n",
    "\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "        self.router_z_loss_coef = router_z_loss_coef\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        noise_gates = False,\n",
    "        noise_mult = 1.\n",
    "    ):\n",
    "        dispatch_tensor, combine_tensor, balance_loss, router_z_loss = self.gate(x, noise_gates = noise_gates, noise_mult = noise_mult)\n",
    "\n",
    "        # dispatch\n",
    "\n",
    "        expert_inputs = einsum('b n d, b n e c -> b e c d', x, dispatch_tensor)\n",
    "\n",
    "        # feed the expert inputs through the experts.\n",
    "\n",
    "        expert_outputs = self.experts(expert_inputs)\n",
    "\n",
    "        # combine\n",
    "\n",
    "        output = einsum('b e c d, b n e c -> b n d', expert_outputs, combine_tensor)\n",
    "\n",
    "        # losses\n",
    "\n",
    "        weighted_balance_loss = balance_loss * self.balance_loss_coef\n",
    "        weighted_router_z_loss = router_z_loss * self.router_z_loss_coef\n",
    "\n",
    "        # combine the losses\n",
    "\n",
    "        total_aux_loss = weighted_balance_loss + weighted_router_z_loss\n",
    "\n",
    "        return MixtureOfExpertsReturn(output, total_aux_loss, balance_loss, router_z_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e009fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install beartype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3c843b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maybe_differentiable_topk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      4\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m----> 5\u001b[0m moe \u001b[38;5;241m=\u001b[39m \u001b[43mMoE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example input tensor\u001b[39;00m\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, seq_len, input_dim)\n",
      "File \u001b[0;32m<@beartype(__main__.MoE.__init__) at 0x7f481da87010>:35\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(__beartype_object_139947712097088, __beartype_get_violation, __beartype_conf, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mMoE.__init__\u001b[0;34m(self, dim, num_experts, expert_hidden_mult, threshold_train, threshold_eval, capacity_factor_train, capacity_factor_eval, gating_top_n, balance_loss_coef, router_z_loss_coef, experts, straight_through_dispatch_tensor, differentiable_topk, differentiable_topk_fused, is_distributed, allow_var_seq_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_experts \u001b[38;5;241m=\u001b[39m num_experts\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate \u001b[38;5;241m=\u001b[39m \u001b[43mTopNGating\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgating_top_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_gates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_experts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstraight_through_dispatch_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstraight_through_dispatch_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable_topk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdifferentiable_topk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapacity_factor_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcapacity_factor_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapacity_factor_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcapacity_factor_eval\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m experts \u001b[38;5;241m=\u001b[39m default(experts, \u001b[38;5;28;01mlambda\u001b[39;00m: [Expert(dim \u001b[38;5;241m=\u001b[39m dim, hidden_mult \u001b[38;5;241m=\u001b[39m expert_hidden_mult) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_experts)])\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts \u001b[38;5;241m=\u001b[39m Experts(\n\u001b[1;32m     41\u001b[0m     experts,\n\u001b[1;32m     42\u001b[0m     is_distributed \u001b[38;5;241m=\u001b[39m is_distributed,\n\u001b[1;32m     43\u001b[0m     allow_var_seq_len \u001b[38;5;241m=\u001b[39m allow_var_seq_len\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m<@beartype(__main__.TopNGating.__init__) at 0x7f481da86b90>:76\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(__beartype_getrandbits, __beartype_get_violation, __beartype_conf, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36mTopNGating.__init__\u001b[0;34m(self, dim, num_gates, eps, top_n, threshold_train, threshold_eval, capacity_factor_train, capacity_factor_eval, straight_through_dispatch_tensor, differentiable_topk, differentiable_topk_fused)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_gates \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(dim, num_gates, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdifferentiable_topk \u001b[38;5;241m=\u001b[39m differentiable_topk\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopk \u001b[38;5;241m=\u001b[39m partial(\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mmaybe_differentiable_topk\u001b[49m,\n\u001b[1;32m     28\u001b[0m     non_differentiable \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m differentiable_topk,\n\u001b[1;32m     29\u001b[0m     fused \u001b[38;5;241m=\u001b[39m differentiable_topk_fused \u001b[38;5;66;03m# use triton fused coordinate descent if possible by default\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m top_n \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmust be 2 or more experts\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_n \u001b[38;5;241m=\u001b[39m top_n\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maybe_differentiable_topk' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 4\n",
    "input_dim = 8\n",
    "hidden_dim = 32\n",
    "moe = MoE(dim=input_dim)\n",
    "\n",
    "# Example input tensor\n",
    "x = torch.randn(batch_size, seq_len, input_dim)\n",
    "moe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14202bec",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 1 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output_original)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mtest_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 94\u001b[0m, in \u001b[0;36mtest_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m model_new \u001b[38;5;241m=\u001b[39m SMoE(input_dim, hidden_dim, num_experts)\n\u001b[1;32m     92\u001b[0m model_original \u001b[38;5;241m=\u001b[39m SMoEOriginal(input_dim, hidden_dim, num_experts)\n\u001b[0;32m---> 94\u001b[0m output_new, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m output_original, _ \u001b[38;5;241m=\u001b[39m model_original(x)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Check if the outputs are close\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m, in \u001b[0;36mSMoE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m selected_expert_outputs \u001b[38;5;241m=\u001b[39m selected_expert_outputs\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m2\u001b[39m, topk_indices\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))  \u001b[38;5;66;03m# (batch_size, seq_len, 2, input_dim)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Correct einsum notation\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m expert_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbsid,bsi->bsd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_expert_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, seq_len, input_dim)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expert_outputs, (topk_scores, topk_indices, gate_scores)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 1 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import os\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class SMoE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts):\n",
    "        super(SMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        self.gating_network = GatingNetwork(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        gate_scores = self.gating_network(x)\n",
    "        topk_scores, topk_indices = torch.topk(gate_scores, 2, dim=-1)\n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Compute expert outputs for all experts in parallel\n",
    "        all_expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2)  # (batch_size, seq_len, num_experts, input_dim)\n",
    "        \n",
    "        # Select top-k expert outputs\n",
    "        selected_expert_outputs = all_expert_outputs.gather(2, topk_indices.unsqueeze(-1).expand(-1, -1, -1, x.size(-1)))  # (batch_size, seq_len, 2, input_dim)\n",
    "        \n",
    "        # Calculate weights and apply them\n",
    "        weights = F.softmax(topk_scores, dim=-1).unsqueeze(-1)  # (batch_size, seq_len, 2, 1)\n",
    "        expert_outputs = (selected_expert_outputs * weights).sum(dim=2)  # (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        return expert_outputs, (topk_scores, topk_indices, gate_scores)\n",
    "\n",
    "class SMoEOriginal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts):\n",
    "        super(SMoEOriginal, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        self.gating_network = GatingNetwork(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        gate_scores = self.gating_network(x)\n",
    "        topk_scores, topk_indices = torch.topk(gate_scores, 2, dim=-1)\n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        expert_outputs = torch.zeros_like(x).to(device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                indices = topk_indices[i, j]\n",
    "                weights = F.softmax(topk_scores[i, j], dim=-1)\n",
    "                output = sum(weights[k] * self.experts[indices[k].item()](x[i:i+1, j:j+1, :]) for k in range(2))\n",
    "                expert_outputs[i:i+1, j:j+1, :] = output\n",
    "        \n",
    "        return expert_outputs, (topk_scores, topk_indices, gate_scores)\n",
    "\n",
    "def test_models():\n",
    "    input_dim = 128\n",
    "    hidden_dim = 256\n",
    "    num_experts = 8\n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "    model_new = SMoE(input_dim, hidden_dim, num_experts)\n",
    "    model_original = SMoEOriginal(input_dim, hidden_dim, num_experts)\n",
    "\n",
    "    output_new, _ = model_new(x)\n",
    "    output_original, _ = model_original(x)\n",
    "\n",
    "    # Check if the outputs are close\n",
    "    if torch.allclose(output_new, output_original, atol=1e-6):\n",
    "        print(\"The outputs are the same for both methods.\")\n",
    "    else:\n",
    "        print(\"The outputs differ between the methods.\")\n",
    "\n",
    "    print(\"Output new method:\")\n",
    "    print(output_new)\n",
    "    print(\"Output original method:\")\n",
    "    print(output_original)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb8431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
